---
title: "Individual Fairness Model"
author: "Simon Schrauth"
date: "`r Sys.Date()`"
output: html_document
---
# Individual

## Goal

After executing this script, the individual fairness/discrimination of the base model should be determined.

## Preparation
### Install packages
```{r}
pacman::p_load(tidyverse, 
               here,
               tidymodels,
               cluster,
               scales,
               MLmetrics)

tidymodels_prefer()
```

### Load data
```{r}
data = readRDS(here("01_data", "02_data_processed", "01_base_model",
                    "03_data.rds"))

model_fit = readRDS(file = here("01_data", "02_data_processed", "01_base_model",
                                "04_model_fit.rds"))
```

## Model Calculation
### Functions
Function for pair-wise distance calculations and comparison of instance & probability distances
```{r}
metric_fun = function(x_high, x_low, y_high, y_low){
      quot1 = log(max(x_low/y_low, y_low/x_low))
      quot2 = log(max(x_high/y_high, y_high/x_high))
      
      output = max(quot1, quot2)
      
      return(output)
      }

diff_dist = function(prob, data){
  
  # D(Mx,My) (probability distance)
  ## define vectors for probabilities of getting a high or low COMPAS score
  prob_high = as.vector(prob$high)
  prob_low = as.vector(prob$low)
  
  ## empty matrix for pair-wise probability distance
  Ddist = matrix(data = NA,
                 nrow = nrow(prob),
                 ncol = nrow(prob))
  
  ## loop over lower triangle matrix of Ddist and calculate pair-wise distances
  for (i in 1:nrow(prob)) {
    for (j in 1:i) {
      x_high = prob_high[i]
      x_low = prob_low[i]
      
      y_high = prob_high[j]
      y_low = prob_low[j]
    
      Ddist[i,j] = metric_fun(x_high, x_low, y_high, y_low)
    }
  }
  
  ## normalise distances to a range of 0 to 1 
  Ddist = rescale(Ddist, to = c(0,1))


  # d(x,y) (instance distance)
  ## calculate pair-wise gower distance
  ddist = data %>% 
    select(!compas) %>% 
    daisy(metric = "gower") %>% 
    as.matrix()
  
  ## create lower triangle matrix for compatibility
  ddist[upper.tri(ddist)] = NA
  
  colnames(ddist) = NULL
  rownames(ddist) = NULL
  
  
  # Difference Ddist & ddist
  ## count the cases where individual fairness is not given
  diff_dist = ifelse(Ddist > ddist, 1, 0)
  
  ## calculate relative proportion of those cases w.r.t all pair-wise distances
  n_unfair = sum(diff_dist, na.rm = TRUE)
  n_complete = (length(diff_dist)/2)-nrow(diff_dist)
  
  n_unfair_rel = n_unfair/n_complete
  n_unfair_rel
  
  # Output
  output = list(diff_dist = diff_dist,
                n_unfair_rel = n_unfair_rel)
  
  return(output)
}
```

Function for basic logistic regression by hand (with manually coded optimization algorithm analog to glm)
```{r}
my_method <- function(x, y, 
                      family = binomial(link = "logit"), 
                      control, ...) {
  # Initialize the beta coefficients to zero
  beta = rep(0, ncol(x))
  
  # Set the maximum number of iterations to 100
  max_iter = 100
  
  # Set the convergence threshold to 1e-6
  tol = 1e-6

  # Iterate until convergence or maximum number of iterations reached
  for (iter in 1:max_iter) {
    # Calculate the predicted probabilities using the current beta coefficients
    eta = x %*% beta
    mu = family$linkinv(eta)
    
    # Calculate the working response
    w = mu * (1 - mu)
    z = eta + (y - mu) / w
    
    # Compute the gradient of the loss function
    grad = t(x) %*% (w * z)
    
    beta_new = beta + (solve(t(x) %*% diag(rep(w, nrow = nrow(x))) %*% x) %*% grad)

    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      break
    }
    
    if(iter%%10 == 0) print(paste("Iteration:", iter))
    
    # Update beta and continue iterating
    beta = beta_new
  }
  
  # calculate predicted values, covariance matrix, standard error of coefficients, z-values and p-values
  p_hat = plogis(x %*% beta)
  V = solve(t(x) %*% diag(c(p_hat * (1 - p_hat))) %*% x)
  se = sqrt(diag(V))
  z = beta / se
  p = 2 * (1 - pnorm(abs(z)))
  
  # Return the coefficients, standard errors, z-values, and p-values as a list
  return(list(coefficients = beta, se = se, z = z, p = p))
}

my_predict = function(summary, data){
  coeffs = as.vector(summary$estimate)
  data_new = data %>%
    select(!c(compas)) %>%
    mutate(intercept = 1) %>%
    relocate(intercept, .before = 0) %>%
    as.matrix()
  
  prob_high = plogis(data_new %*% coeffs)
  
  prob_low = 1 - prob_high
  
  df = data.frame(low = prob_low,
                  high = prob_high)
  
  return(df)
}
```

### Estimate model with custom optimization algorithm
```{r}
data_prep = recipe(compas ~ .,
                         data = data) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  mutate(compas = as.numeric(compas)-1)

model_base = glm(compas ~ ., 
                 data = data_prep, 
                 family = binomial(link = "logit"),
                 method = my_method)

model_base_summary = data.frame(estimate = model_base$coefficients,
                                se = as.vector(model_base$se),
                                z = model_base$z,
                                p_value = model_base$p)

model_base_summary %>% 
    slice(-1) %>% 
    mutate(estimate = exp(estimate)) %>% 
    mutate(estimate_percent = percent((estimate - 1), digits = 1)) %>% 
    relocate(estimate_percent, .after = estimate)
```

### Calculate individual discrimination
```{r}
# extract probabilities out of base model
base_prob = my_predict(summary = model_base_summary, 
                       data = data_prep)

# calculate proportion of cases where individual fairness is not given (D(Mx, My) > d(x,y))
dist_base = diff_dist(prob = base_prob, 
                      data = data)

dist_base$n_unfair_rel
```

### Save output
```{r}
saveRDS(model_base,
        file = here("01_data", "02_data_processed", "02_individual", 
                    "01_model_base.rds"))

saveRDS(dist_base,
        file = here("01_data", "02_data_processed", "02_individual", "big_data",
                    "01_dist_base.rds"))
```
```{r}
model_base = readRDS(file = here("01_data", "02_data_processed", "02_individual", 
                                 "01_model_base.rds"))

dist_base = readRDS(file = here("01_data", "02_data_processed", "02_individual", "big_data",
                                "01_dist_base.rds"))
```


## In-Processing
### Functions
New loss function
```{r}
my_penalty_function <- function(mu, x, metric) {
  
  prob = data.frame(low = mu,
                    high = 1-mu)
  
  data = x[,2:ncol(x)] %>% 
    as.data.frame() %>% 
    mutate(compas = NA) %>% 
    mutate(sex = factor(ifelse(sex_Female == 1, "Female", "Male"))) %>% 
    mutate(race = factor(case_when(
                                    race_African.American == 1 ~ "African American",
                                    race_Hispanic == 1 ~ "Hispanic",
                                    race_Other == 1 ~ "Other",
                                    .default = "Caucasian"
                                  )
                         )
           ) %>% 
    mutate(charge_degree = factor(ifelse(charge_degree_M == 1, "M", "F"))) %>% 
    mutate(two_year_recid = factor(ifelse(two_year_recid_yes == 1, "yes", "no"))) %>% 
    select(sex,
           race,
           age,
           days_in_jail,
           juv_fel_count,
           juv_misd_count,
           juv_other_count,
           priors_count,
           charge_degree,
           two_year_recid,
           compas)

  dist = diff_dist(prob = prob, data = data)

  penalty = dist$n_unfair_rel
  
  return(penalty)
}
```

Function for logistic regression with penalty by hand 
```{r}
my_method_with_penalty <- function(x, y, 
                                   family = binomial(link = "logit"), 
                                   control, ...) {
  # Initialize the beta coefficients to zero
  beta = rep(0, ncol(x))
  
  # Set the maximum number of iterations to 100
  max_iter = 10 ##########!!!!!!!!!!!!!!!!!
  
  # Set the convergence threshold to 1e-6
  tol = 1e-6

  # Set the penalty weight to 0.5
  lambda = control$lambda
  
  metric = control$metric
  
  # Set the penalty function
  penalty_function = control$penalty_function
  
  # Iterate until convergence or maximum number of iterations reached
  for (iter in 1:max_iter) {
    # Calculate the predicted probabilities using the current beta coefficients
    eta = x %*% beta
    mu = family$linkinv(eta)
    
    # Calculate the working response
    w = mu * (1 - mu)
    z = eta + (y - mu) / w
    
    # Compute the gradient of the loss function
    grad = t(x) %*% (w * z)
    
    # Compute the penalty term gradient
    pen_grad = penalty_function(mu = mu, x = x)
    
    # Compute the total gradient and update the beta coefficients
    total_grad = grad + lambda * pen_grad
    
    beta_new = beta + (solve(t(x) %*% diag(rep(w, nrow = nrow(x))) %*% x) %*% total_grad)

    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      break
    }
    
    if(iter%%10 == 0) print(paste("Iteration:", iter)) 
    
    # Update beta and continue iterating
    beta = beta_new
  }
  
  # calculate predicted values, covariance matrix, standard error of coefficients, z-values and p-values
  p_hat = plogis(x %*% beta)
  V = solve(t(x) %*% diag(c(p_hat * (1 - p_hat))) %*% x)
  se = sqrt(diag(V))
  z = beta / se
  p = 2 * (1 - pnorm(abs(z)))
  
  # Return the coefficients, standard errors, z-values, and p-values as a list
  return(list(coefficients = beta, se = se, z = z, p = p))
}
```

### Hyper-Parameter Tuning (lambda)
```{r}
lambdas = seq(-150, 0, 75)

results_fair = c()

results_acc = c()

for (lambda in lambdas) {
  print(paste("Lambda:", lambda))
  
  model_temp = glm(compas ~ ., 
                   data = data_prep, 
                   family = binomial(link = "logit"),
                   method = my_method_with_penalty,
                   control = list(lambda = lambda,
                                  penalty_function = my_penalty_function))

  model_temp_summary = data.frame(estimate = model_temp$coefficients,
                                  se = as.vector(model_temp$se),
                                  z = model_temp$z,
                                  p_value = model_temp$p)
  
  pen_prob = my_predict(summary = model_temp_summary, 
                        data = data_prep)
  

  dist_linf_pen = diff_dist(prob = pen_prob, 
                            data = data)
  
  logloss = LogLoss(pen_prob$low, data_prep$compas)
  
  results_fair = append(results_fair, values = dist_linf_pen$n_unfair_rel)
  results_acc = append(results_acc, values = logloss)
}

lambdas
results_fair
results_acc

hyperparam_results = data.frame(lambda = lamdas,
                                fairness = results_fair,
                                logloss = results_acc)

hyperparam_results

lambda = c(1,10, 100, -1, -10, -100)
results_fair = c(0.1657097, 0.1769129, 0.3311039,  0.1631485, 0.1513733, 0.09468478)
results_acc = c(2.157076, 2.153654, 2.094676, 2.157982, 2.163087, 2.589211)

```

### Final fair model
```{r}
model_fair = glm(compas ~ ., 
                 data = data_prep, 
                 family = binomial(link = "logit"),
                 method = my_method_with_penalty,
                 control = list(lambda = 1, 
                                penalty_function = my_penalty_function))

model_fair_summary = data.frame(estimate = model_fair$coefficients,
                                se = as.vector(model_fair$se),
                                z = model_fair$z,
                                p_value = model_fair$p)

model_fair_summary %>% 
    slice(-1) %>% 
    mutate(estimate = exp(estimate)) %>% 
    mutate(estimate_percent = percent((estimate - 1), digits = 1)) %>% 
    relocate(estimate_percent, .after = estimate)

```

Calculate individual discrimination
```{r}
# extract probabilities out of base model
pen_prob = my_predict(summary = model_fair_summary, 
                      data = data_prep)

# calculate proportion of cases where individual fairness is not given (D(Mx, My) > d(x,y))
dist_pen = diff_dist(prob = pen_prob, 
                          data = data)

dist_pen$n_unfair_rel
```

Save output
```{r}
saveRDS(model_fair,
        file = here("01_data", "02_data_processed", "02_individual", 
                    "02_model_fair.rds"))

saveRDS(dist_pen,
        file = here("01_data", "02_data_processed", "02_individual", "big_data",
                    "02_dist_pen.rds"))
```

```{r}
model_fair = readRDS(file = here("01_data", "02_data_processed", "02_individual", 
                                 "02_model_fair.rds"))

dist_pen = readRDS(file = here("01_data", "02_data_processed", "02_individual", "big_data",
                                    "02_dist_pen.rds"))
```


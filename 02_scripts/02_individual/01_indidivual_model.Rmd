---
title: "Individual Fairness Model"
author: "Simon Schrauth"
date: "`r Sys.Date()`"
output: html_document
---
# Individual

## Goal

After executing this script, the individual fairness/discrimination of the base model should be determined.

## Preparation
### Install packages
```{r}
pacman::p_load(tidyverse, 
               here,
               tidymodels,
               cluster,
               scales,
               MLmetrics)

tidymodels_prefer()
```

### Load data
```{r}
data = readRDS(here("01_data", "02_data_processed", "01_base_model",
                    "03_data.rds"))

model_fit = readRDS(file = here("01_data", "02_data_processed", "01_base_model",
                                "04_model_fit.rds"))
```

## Model Calculation
### Functions
Function for pair-wise distance calculations and comparison of instance & probability distances
```{r}
metric_stat_fun = function(x_high, x_low, y_high, y_low){
      output = (1/2)*sum(abs(x_low-y_low), abs(x_high-y_high))
      
      return(output)
      }

metric_linf_fun = function(x_high, x_low, y_high, y_low){
      quot1 = log(max(x_low/y_low, y_low/x_low))
      quot2 = log(max(x_high/y_high, y_high/x_high))
      
      output = max(quot1, quot2)
      
      return(output)
      }

diff_dist = function(base_prob, data, metric = "statistic"){
  
  # D(Mx,My) (probability distance)
  ## define metric function as statistical distance or relative l-infinity metric
  if(metric == "statistic"){
    metric_fun = metric_stat_fun
    
  } else if (metric == "l_inf"){
    metric_fun = metric_linf_fun
    
    } else {
      return(warning("metric must be set to 'statistic' or 'l_inf'"))
    }
  
  ## define vectors for probabilities of getting a high or low COMPAS score
  base_prob_high = as.vector(base_prob$high)
  base_prob_low = as.vector(base_prob$low)
  
  ## empty matrix for pair-wise probability distance
  Ddist = matrix(data = NA,
                 nrow = nrow(base_prob),
                 ncol = nrow(base_prob))
  
  ## loop over lower triangle matrix of Ddist and calculate pair-wise distances
  for (i in 1:nrow(base_prob)) {
    for (j in 1:i) {
      x_high = base_prob_high[i]
      x_low = base_prob_low[i]
      
      y_high = base_prob_high[j]
      y_low = base_prob_low[j]
    
      Ddist[i,j] = metric_fun(x_high, x_low, y_high, y_low)
    }
  }
  
  ## normalise distances to a range of 0 to 1 
  if(metric == "l_inf"){
    Ddist = rescale(Ddist, to = c(0,1))
  }
  
  
  # d(x,y) (instance distance)
  ## calculate pair-wise gower distance
  ddist = data %>% 
    select(!compas) %>% 
    daisy(metric = "gower") %>% 
    as.matrix()
  
  ## create lower triangle matrix for compatibility
  ddist[upper.tri(ddist)] = NA
  
  colnames(ddist) = NULL
  rownames(ddist) = NULL
  
  
  # Difference Ddist & ddist
  ## count the cases where individual fairness is not given
  diff_dist = ifelse(Ddist > ddist, 1, 0)
  
  ## calculate relative proportion of those cases w.r.t all pair-wise distances
  n_unfair = sum(diff_dist, na.rm = TRUE)
  n_complete = (length(diff_dist)/2)-nrow(diff_dist)
  
  n_unfair_rel = n_unfair/n_complete
  n_unfair_rel
  
  # Output
  output = list(Ddist = Ddist, 
                ddist = ddist, 
                diff_dist = diff_dist,
                n_unfair_rel = n_unfair_rel)
  
  return(output)
}
```

Function for basic logistic regression by hand (with manually coded optimization algorithm analog to glm)
```{r}
my_method <- function(x, y, 
                      family = binomial(link = "logit"), 
                      control, ...) {
  # Initialize the beta coefficients to zero
  beta = rep(0, ncol(x))
  
  # Set the maximum number of iterations to 100
  max_iter = 100
  
  # Set the convergence threshold to 1e-6
  tol = 1e-6

  # Iterate until convergence or maximum number of iterations reached
  for (iter in 1:max_iter) {
    # Calculate the predicted probabilities using the current beta coefficients
    eta = x %*% beta
    mu = family$linkinv(eta)
    
    # Calculate the working response
    w = mu * (1 - mu)
    z = eta + (y - mu) / w
    
    # Compute the gradient of the loss function
    grad = t(x) %*% (w * z)
    
    beta_new = beta + (solve(t(x) %*% diag(rep(w, nrow = nrow(x))) %*% x) %*% grad)

    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      break
    }
    
    if(iter%%10 == 0) print(paste("Iteration:", iter))
    
    # Update beta and continue iterating
    beta = beta_new
  }
  
  # calculate predicted values, covariance matrix, standard error of coefficients, z-values and p-values
  p_hat = plogis(x %*% beta)
  V = solve(t(x) %*% diag(c(p_hat * (1 - p_hat))) %*% x)
  se = sqrt(diag(V))
  z = beta / se
  p = 2 * (1 - pnorm(abs(z)))
  
  # Return the coefficients, standard errors, z-values, and p-values as a list
  return(list(coefficients = beta, se = se, z = z, p = p))
}

my_predict = function(summary, data){
  coeffs = as.vector(summary$estimate)
  data_new = data %>%
    select(!c(compas)) %>%
    mutate(intercept = 1) %>%
    relocate(intercept, .before = 0) %>%
    as.matrix()
  
  prob_high = plogis(data_new %*% coeffs)
  
  prob_low = 1 - prob_high
  
  df = data.frame(low = prob_low,
                  high = prob_high)
  
  return(df)
}
```

### Estimate model with custom optimization algorithm
```{r}
data_prep = recipe(compas ~ .,
                         data = data) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  prep() %>% 
  bake(new_data = NULL)

model_base = glm(compas ~ ., 
                 data = data_prep, 
                 family = binomial(link = "logit"),
                 method = my_method)

model_base_summary = data.frame(estimate = model_base$coefficients,
                                se = as.vector(model_base$se),
                                z = model_base$z,
                                p_value = model_base$p)

model_base_summary %>% 
    slice(-1) %>% 
    mutate(estimate = exp(estimate)) %>% 
    mutate(estimate_percent = percent((estimate - 1), digits = 1)) %>% 
    relocate(estimate_percent, .after = estimate)
```

### Calculate individual discrimination
```{r}
# extract probabilities out of base model
base_prob = my_predict(summary = model_base_summary, 
                       data = data_prep)

# calculate proportion of cases where individual fairness is not given (D(Mx, My) > d(x,y))
dist_stat_base = diff_dist(base_prob = base_prob, 
                           data = data, 
                           metric = "statistic")

dist_linf_base = diff_dist(base_prob = base_prob, 
                           data = data, 
                           metric = "l_inf")

dist_stat_base$n_unfair_rel
dist_linf_base$n_unfair_rel
```

### Save output
```{r}
saveRDS(model_base,
        file = here("01_data", "02_data_processed", "02_individual", 
                    "01_model_base.rds"))

saveRDS(dist_stat_base,
        file = here("01_data", "02_data_processed", "02_individual", "big_data",
                    "01_dist_stat_base.rds"))

saveRDS(dist_linf_base,
        file = here("01_data", "02_data_processed", "02_individual", "big_data",
                    "02_dist_linf_base.rds"))

model_base = readRDS(file = here("01_data", "02_data_processed", "02_individual", 
                                 "01_model_base.rds"))

dist_stat_base = readRDS(file = here("01_data", "02_data_processed", "02_individual", "big_data",
                                     "01_dist_stat_base.rds"))

dist_linf_base = readRDS(file = here("01_data", "02_data_processed", "02_individual", "big_data",
                                     "02_dist_linf_base.rds"))
```



## In-Processing
### Functions
New loss function
```{r}
my_penalty_function <- function(x, y, beta, prob, metric_fun) {
  
  # calculate pair-wise distance of actual y's
  ## define vectors for probabilities of getting a high or low COMPAS score
  prob_high = as.vector(prob$high)
  prob_low = as.vector(prob$low)
  
  ## empty matrix for pair-wise probability distance
  Ddist = matrix(data = NA,
                 nrow = length(prob_high),
                 ncol = length(prob_high))
  
  ## loop over lower triangle matrix of Ddist and calculate pair-wise distances
  for (i in 1:length(prob_high)) {
    for (j in 1:i) {
      x_high = prob_high[i]
      x_low = prob_low[i]
      
      y_high = prob_high[j]
      y_low = prob_low[j]
    
      Ddist[i,j] = metric_fun(x_high, x_low, y_high, y_low)
    }
  }
  
  diag(Ddist) = NA
  
  # calculate squared pair-wise distance of predicted y's (y_hat)
  ## define vector for predicted probabilities of getting a high COMPAS score
  prob_high_hat = as.vector(plogis(x %*% beta))

  ## empty matrix for pair-wise probability distance
  Ddist_hat = matrix(data = NA,
                 nrow = length(prob_high_hat),
                 ncol = length(prob_high_hat))
  
  ## loop over lower triangle matrix of Ddist_hat and calculate pair-wise distances
  for (i in 1:length(prob_high_hat)) {
    for (j in 1:i) {
      x_high_hat = prob_high_hat[i]

      y_high_hat = prob_high_hat[j]

      Ddist_hat[i,j] = (x_high_hat-y_high_hat)^2
    }
  }
  
  diag(Ddist_hat) = NA
  
  # calculate fairness penalty
  ## summands as multiplication of pair-wise distance of actual and predicted y's
  summands = Ddist*Ddist_hat
  
  ## penalty as average of all multiplicated pair-wise distances
  penalty = summands %>% 
    as.vector() %>% 
    na.omit() %>% 
    mean()

  return(penalty)
}
```

Function for logistic regression with penalty by hand 
```{r}
my_method_with_penalty <- function(x, y, 
                                   family = binomial(link = "logit"), 
                                   control, ...) {
  # Initialize the beta coefficients to zero
  beta = rep(0, ncol(x))
  
  # Set the maximum number of iterations to 100
  max_iter = 100 
  
  # Set the convergence threshold to 1e-6
  tol = 1e-6

  # Set the penalty weight to 0.5
  lambda = control$lambda
  
  prob = control$prob
  
  metric_fun = control$metric_fun
  
  # Set the penalty function
  penalty_function = control$penalty_function
  
  # Iterate until convergence or maximum number of iterations reached
  for (iter in 1:max_iter) {
    # Calculate the predicted probabilities using the current beta coefficients
    eta = x %*% beta
    mu = family$linkinv(eta)
    
    # Calculate the working response
    w = mu * (1 - mu)
    z = eta + (y - mu) / w
    
    # Compute the gradient of the loss function
    grad = t(x) %*% (w * z)
    
    # Compute the penalty term gradient
    pen_grad = penalty_function(x, y, beta, prob, metric_fun)
    
    # Compute the total gradient and update the beta coefficients
    total_grad = grad + lambda * pen_grad
    
    beta_new = beta + (solve(t(x) %*% diag(rep(w, nrow = nrow(x))) %*% x) %*% total_grad)

    # Check for convergence
    if (max(abs(beta_new - beta)) < tol) {
      break
    }
    
    if(iter%%10 == 0) print(paste("Iteration:", iter)) 
    
    # Update beta and continue iterating
    beta = beta_new
  }
  
  # calculate predicted values, covariance matrix, standard error of coefficients, z-values and p-values
  p_hat = plogis(x %*% beta)
  V = solve(t(x) %*% diag(c(p_hat * (1 - p_hat))) %*% x)
  se = sqrt(diag(V))
  z = beta / se
  p = 2 * (1 - pnorm(abs(z)))
  
  # Return the coefficients, standard errors, z-values, and p-values as a list
  return(list(coefficients = beta, se = se, z = z, p = p))
}
```

### Hyper-Parameter Tuning (lambda)
```{r}
lambdas = c(1, 10, 100)

results_fair = c()

results_acc = c()

lambda = 2.4

for (lambda in lambdas) {
  print(paste("Lambda:", lambda))
  
  model_temp = glm(compas ~ ., 
                   data = data_prep, 
                   family = binomial(link = "logit"),
                   method = my_method_with_penalty,
                   control = list(lambda = lambda, 
                                  prob = prob,
                                  metric_fun = metric_linf_fun,
                                  penalty_function = my_penalty_function))

  model_temp_summary = data.frame(estimate = model_temp$coefficients,
                                  se = as.vector(model_temp$se),
                                  z = model_temp$z,
                                  p_value = model_temp$p)
  
  pen_prob = my_predict(summary = model_fair_summary, 
                        data = data_prep)
  

  dist_linf_pen = diff_dist(base_prob = pen_prob, 
                            data = data, 
                            metric = "l_inf")
  
  library(MLmetrics)
  logloss = LogLoss(pen_prob$high, data_prep$compas)
  
  results_fair = append(results_fair, values = dist_linf_pen$n_unfair_rel)
  results_acc = append(results_acc, values = logloss)
}
lambda = c(1,)
results_fair = c()
results_acc = c()
```

### Final fair model
```{r}
model_fair = glm(compas ~ ., 
                 data = data_prep, 
                 family = binomial(link = "logit"),
                 method = my_method_with_penalty,
                 control = list(lambda = 1, 
                                prob = prob,
                                #metric_fun = metric_stat_fun, 
                                metric_fun = metric_linf_fun,
                                penalty_function = my_penalty_function))

model_fair_summary = data.frame(estimate = model_fair$coefficients,
                                se = as.vector(model_fair$se),
                                z = model_fair$z,
                                p_value = model_fair$p)

model_fair_summary %>% 
    slice(-1) %>% 
    mutate(estimate = exp(estimate)) %>% 
    mutate(estimate_percent = percent((estimate - 1), digits = 1)) %>% 
    relocate(estimate_percent, .after = estimate)

```

Calculate individual discrimination
```{r}
# extract probabilities out of base model
pen_prob = my_predict(summary = model_fair_summary, 
                      data = data_prep)

# calculate proportion of cases where individual fairness is not given (D(Mx, My) > d(x,y))
dist_stat_pen = diff_dist(base_prob = pen_prob, 
                          data = data, 
                          metric = "statistic")

dist_linf_pen = diff_dist(base_prob = pen_prob, 
                          data = data, 
                          metric = "l_inf")

dist_stat_pen$n_unfair_rel
dist_linf_pen$n_unfair_rel
```

Save output
```{r}
saveRDS(model_fair,
        file = here("01_data", "02_data_processed", "02_individual", 
                    "02_model_fair.rds"))

saveRDS(dist_stat_pen,
        file = here("01_data", "02_data_processed", "02_individual", "big_data",
                    "03_dist_stat_pen.rds"))

saveRDS(dist_linf_pen,
        file = here("01_data", "02_data_processed", "02_individual", "big_data",
                    "04_dist_linf_pen.rds"))

model_fair = readRDS(file = here("01_data", "02_data_processed", "02_individual", 
                                 "02_model_fair.rds"))

dist_stat_pen = readRDS(file = here("01_data", "02_data_processed", "02_individual", "big_data",
                                    "03_dist_stat_pen.rds"))

dist_linf_pen = readRDS(file = here("01_data", "02_data_processed", "02_individual", "big_data",
                                    "04_dist_linf_pen.rds"))
```

